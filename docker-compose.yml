services:
  openWebUI:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui-nvidia-docker
    hostname: openwebui
    networks:
      - skynet
    restart: unless-stopped
    volumes:
      - openWebUI-data:/app/backend/data
    ports:
      - 8080:8080
    environment:
      OLLAMA_BASE_URLS: http://ollama:11434
    depends_on:
      - ollama
    deploy:
      replicas: ${START_OLLAMA_RELATED_SERVICES:-0}

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-nvidia-docker
    hostname: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              count: all
      replicas: ${START_OLLAMA_RELATED_SERVICES:-0}
    networks:
      - skynet
    ports:
      - 11434:11434
    volumes:
      - ollama-data:/root/.ollama

  # latest script
  automatic1111:
    container_name: automatic1111-nvidia-docker
    image: automatic1111:latest
    restart: always
    build:
      context: .
      dockerfile: Automatic111.dockerfile
      args:
        # Pass .env vars
        INSTALL_AUTOMATIC_EXTENTIONS: ${INSTALL_AUTOMATIC_EXTENTIONS}
    ports:
      - "7860:7860"
    depends_on:
      - build-cuda122
    # environment:
    #   PUID: 1000
    #   PGID: 1000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              count: all
      replicas: ${AUTOMATIC_ACTIVE:-0}
    networks:
      - skynet
    volumes:
      - automatic1111-outputs:/home/appuser/stable-diffusion-webui/outputs
      - automatic1111-models:/home/appuser/stable-diffusion-webui/models/
      - automatic1111-embeddings:/home/appuser/stable-diffusion-webui/embeddings/
      - automatic1111-extentions:/home/appuser/stable-diffusion-webui/extensions

  comfyui:
    container_name: comfyui-nvidia-docker
    image: comfyui:latest
    restart: always
    build:
      context: .
      dockerfile: Confyui.dockerfile
      args:
        # Pass .env vars
        INSTALL_COMFYUI_MANAGER: ${INSTALL_COMFYUI_MANAGER}
    ports:
      - "7861:8188"
    depends_on:
      - build-cuda122
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              count: all
      replicas: ${COMFYUI_ACTIVE:-0}
    networks:
      - skynet
    volumes:
      - comfyui-outputs:/home/appuser/ComfyUI/outputs
      - comfyui-models:/home/appuser/ComfyUI/models/
      - automatic1111-models:/home/appuser/sd/models/:ro
      - automatic1111-embeddings:/home/appuser/sd/embeddings:ro

  roop:
    container_name: roop-nvidia-docker
    image: roop:latest
    restart: always
    build:
      context: .
      dockerfile: Roop.dockerfile
    ports:
      - "7862:7860"
    depends_on:
      - build-cuda122
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              count: all
      replicas: ${ROOP_ACTIVE:-0}
    networks:
      - skynet
    #volumes:
    #  - roop-output:/home/appuser/roop-unleashed/output

  forge:
    container_name: forge-nvidia-docker
    image: forge:latest
    restart: always
    build:
      context: .
      dockerfile: Forge.dockerfile
    ports:
      - "7866:7860"
    depends_on:
      - build-cuda122
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              count: all
      #replicas: ${AUTOMATIC_ACTIVE:-0}
    networks:
      - skynet
    volumes:
      - automatic1111-outputs:/home/appuser/forge/outputs
      - automatic1111-models:/home/appuser/forge/models/
      - automatic1111-embeddings:/home/appuser/forge/embeddings/
      #- automatic1111-extentions:/home/appuser/forge/extensions



  flux-gym:
     container_name: flux-gym-nvidia-docker
     image: flux-gym-nvidia-docker
     build:
       context: .
       dockerfile: Fluxgym.dockerfile
     ports:
       - 7863:7860
     volumes:
       - flux-gym-data:/app/data
       - flux-gym-outputs/:/app/fluxgym/outputs/
     deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               capabilities: [gpu]
               count: all
       replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  # wd-llm-caption-cli:
  #   container_name: wd-llm-caption-cli-nvidia-docker
  #   image: wd-llm-caption-cli
  #   build:
  #     context: .
  #     dockerfile: Caption.dockerfile
  #   ports:
  #     - 7864:8282
  #   volumes:
  #     - wd-llm-caption-data:/home/appuser/data
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: all
  #     replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  # XXXX # kohya_ss with flux1 support
  # kohya-ss-gui:
  #   container_name: kohya-ss-gui-nvidia-docker
  #   image: kohya-ss-gui:sd3-flux.1
  #   build:
  #     context: .
  #     dockerfile: Kohya_ss.dockerfile
  #   ports:
  #     - 7863:7860
  #   environment:
  #     SAFETENSORS_FAST_GPU: 1
  #     TENSORBOARD_PORT: ${TENSORBOARD_PORT:-6006}
  #     PUID: 1000
  #     PGID: 1000
  #     TZ: Europe/Berlin
  #   volumes:
  #     - /home/kilone/space/dockerSpace/kohya_flux/data/:/app
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: all
  #     replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  # XXX # kohya_ss latest
  # kohya-ss-gui-latest:
  #   container_name: kohya-ss-gui-latest
  #   image: ghcr.io/bmaltais/kohya-ss-gui:latest
  #   user: 1000:0
  #   build:
  #     context: .
  #     args:
  #       - UID=1000
  #     cache_from:
  #       - ghcr.io/bmaltais/kohya-ss-gui:cache
  #     cache_to:
  #       - type=inline
  #   ports:
  #     - 7863:7860
  #   environment:
  #     SAFETENSORS_FAST_GPU: 1
  #     TENSORBOARD_PORT: ${TENSORBOARD_PORT:-6006}
  #   tmpfs:
  #     - /tmp
  #   volumes:
  #     - /tmp/.X11-unix:/tmp/.X11-unix
  #     - /home/kilone/space/dockerSpace/kohya_ss/dataset:/dataset
  #     - /home/kilone/space/dockerSpace/kohya_ss/dataset/images:/app/data
  #     - /home/kilone/space/dockerSpace/kohya_ss/dataset/logs:/app/logs
  #     - /home/kilone/space/dockerSpace/kohya_ss/dataset/outputs:/app/outputs
  #     - /home/kilone/space/dockerSpace/kohya_ss/dataset/regularization:/app/regularization
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/config:/app/config
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/user:/home/1000/.cache
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/triton:/home/1000/.triton
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/nv:/home/1000/.nv
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/keras:/home/1000/.keras
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/config:/home/1000/.config # For backward compatibility
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: all
  #     replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  # tensorboard:
  #   container_name: tensorboard-nvidia-docker
  #   image: tensorflow/tensorflow:latest-gpu
  #   ports:
  #     # !Please change the port in .env file
  #     - ${TENSORBOARD_PORT:-6006}:6006
  #   volumes:
  #     - tensorboard-logs:/app/logs
  #   command: tensorboard --logdir=/app/logs --bind_all
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: all
  #     replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  #######################
  # reference containers
  #######################
  build-cuda122:
    build:
      context: .
      dockerfile: Cuda12.2.2.dockerfile
    container_name: build_cuda122
    networks:
      - skynet
    # Kein Starten des Containers: macht nichts außer Build
    entrypoint: ["/bin/true"]
    deploy:
      replicas: 0

  build-cuda118:
    build:
      context: .
      dockerfile: Cuda11.8.0.dockerfile
    container_name: build_cuda118
    networks:
      - skynet
    # Kein Starten des Containers: macht nichts außer Build
    entrypoint: ["/bin/true"]
    deploy:
      replicas: 0

networks:
  skynet:
    external: false

volumes:
  # LLM
  openWebUI-data:
    driver: local
    driver_opts:
      type: none
      device: ${BASE_PATH}${OPENWEBUI_PATH_TO_DATA}
      o: bind
  ollama-data:
    driver: local
    driver_opts:
      type: none
      device: ${BASE_PATH}${OLLAMA_PATH_TO_DATA}
      o: bind
  # Automatic1111
  automatic1111-outputs:
    driver: local
    driver_opts:
      type: none
      device: ${BASE_PATH}${AUTOMATIC_PATH_TO_OUTPUTS}
      o: bind
  automatic1111-models:
    driver: local
    driver_opts:
      type: none
      device: ${BASE_PATH}${AUTOMATIC_PATH_TO_MODELS}
      o: bind
  automatic1111-embeddings:
    driver: local
    driver_opts:
      type: none
      device: ${BASE_PATH}${AUTOMATIC_PATH_TO_EMBEDDINGS}
      o: bind
  automatic1111-extentions:
    driver: local
    driver_opts:
      type: none
      device: ${BASE_PATH}${AUTOMATIC_PATH_TO_EXTENTIONS}
      o: bind
  # ComfyUi
  comfyui-outputs:
    driver: local
    driver_opts:
      type: none
      device: ${BASE_PATH}${COMFYUI_PATH_TO_OUTPUTS}
      o: bind
  comfyui-models:
    driver: local
    driver_opts:
      type: none
      device: ${BASE_PATH}${COMFYUI_PATH_TO_MODELS}
      o: bind
  comfyui-extentions:
    driver: local
    driver_opts:
      type: none
      device: ${BASE_PATH}${COMFYUI_PATH_TO_EXTENTIONS}
      o: bind
  # Roop
  roop-output:
    driver: local
    driver_opts:
      type: none
      device: ${BASE_PATH}${ROOP_PATH_TO_OUTPUTS}
      o: bind
  # Flux-gym
  flux-gym-data:
  flux-gym-outputs:
  # Other
  wd-llm-caption-data:
  tensorboard-logs:
