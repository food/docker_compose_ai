#######################
#    ü§π SERVICES ü§π
#######################
# ‚úîÔ∏è ollama
# ‚úîÔ∏è openweb-ui
# ‚úîÔ∏è stable-deffusion automatic1111 + tools
# ‚úîÔ∏è comfyui INSTALL MANAGER
# üòµ‚Äçüí´ roop output folder not writable
# üí© kohya_ss
# üí© flux-gym
# üí© wd-llm-caption-cli
# üí© tensorboard
# wd-llm-caption-cli for better image captions
# text to speach /whisper / piper
# speach to text
# jupyter   docker run -d -p 8888:8888 --gpus all --name jupxter quay.io/jupyter/pytorch-notebook:cuda12-python-3.11.8
# home assistant support
#######################

#######################
#######################
## Prestuff on host required:
# curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
#  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
# sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
# sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
# sudo apt update
# sudo apt install -y nvidia-container-toolkit
#######################
## Configure NVIDIA Container Toolkit
# sudo nvidia-ctk runtime configure --runtime=docker
# sudo systemctl restart docker
#######################
## Test GPU integration
# docker run --gpus all nvidia/cuda:11.5.2-base-ubuntu20.04 nvidia-smi
#######################
#######################
# Using models
# llama3.2:latest
# (llama3.2-vision:latest)
# qwen2.5-coder:latest
# ?? nemotron: ??
# phi3.5:latest
# llava:latest
# nomic-embed-text:latest
# deepseek-coder-v2:latest
# stable-code:latest
# starcoder2:latest
# (codellama:latest)
# codestral:latest
# llama3:70b
# llama3
# (deepseek-v2:latest)
# (blackhat-hacker:latest)
# (whiterabbitneo:latest )
# orca-mini:latest
# (brxce/stable-diffusion-prompt-generator:latest)
#######################
#######################

services:
  # openWebUI:
  #   image: ghcr.io/open-webui/open-webui:main
  #   container_name: openwebui-nvidia-docker
  #   hostname: openwebui
  #   networks:
  #     - skynet
  #   restart: unless-stopped
  #   volumes:
  #     - openWebUI-data:/app/backend/data
  #   ports:
  #     - 8080:8080
  #   environment:
  #     OLLAMA_BASE_URLS: http://ollama:11434
  #   depends_on:
  #     - ollama
  #   deploy:
  #     replicas: ${START_OLLAMA_RELATED_SERVICES:-0}

  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama-nvidia-docker
  #   hostname: ollama
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: ["gpu"]
  #             count: all
  #     replicas: ${START_OLLAMA_RELATED_SERVICES:-0}
  #   networks:
  #     - skynet
  #   volumes:
  #     - ollama-data:/root/.ollama

  # latest script
  # automatic1111:
  #   container_name: automatic1111-nvidia-docker
  #   image: automatic1111:latest
  #   restart: always
  #   build:
  #     context: .
  #     dockerfile: Automatic111.dockerfile
  #     args:
  #       # Pass .env vars
  #       INSTALL_COMFYUI_MANAGER: ${INSTALL_COMFYUI_MANAGER}
  #   ports:
  #     - "7860:7860"
  #   depends_on:
  #     - build-cuda122
  #   environment:
  #     PUID: 1000
  #     PGID: 1000
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [ gpu ]
  #             count: all
  #     replicas: ${AUTOMATIC_ACTIVE:-0}
  #   networks:
  #     - skynet
  #   volumes:
  #     - automatic1111-outputs:/home/appuser/stable-diffusion-webui/outputs
  #     - automatic1111-models:/home/appuser/stable-diffusion-webui/models/
  #     - automatic1111-embeddings:/home/appuser/stable-diffusion-webui/embeddings/

  # comfyui:
  #   container_name: comfyui-nvidia-docker
  #   image: comfyui:latest
  #   restart: always
  #   build:
  #     context: .
  #     dockerfile: Confyui.dockerfile
  #     args:
  #       # Pass .env vars
  #       INSTALL_COMFYUI_MANAGER: ${INSTALL_COMFYUI_MANAGER}
  #   ports:
  #     - "7861:8188"
  #   depends_on:
  #     - build-cuda122
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [ gpu ]
  #             count: all
  #     replicas: ${COMFYUI_ACTIVE:-0}
  #   networks:
  #     - skynet
  #   volumes:
  #     - comfyui-outputs:/home/appuser/ComfyUI/outputs
  #     - comfyui-models:/home/appuser/ComfyUI/models/
  #     - automatic1111-models:/home/appuser/sd/models/:ro
  #     - automatic1111-embeddings:/home/appuser/sd/embeddings:ro

  roop:
    container_name: roop-nvidia-docker
    image: roop:latest
    restart: always
    build:
      context: .
      dockerfile: Roop.dockerfile
    ports:
      - "7862:7860"
    depends_on:
      - build-cuda122
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              count: all
      replicas: ${ROOP_ACTIVE:-0}
    networks:
      - skynet
    #volumes:
    #  - roop-output:/home/appuser/roop-unleashed/output

  # flux-gym:
  #   container_name: flux-gym-nvidia-docker
  #   image: flux-gym-nvidia-docker
  #   build:
  #     context: .
  #     dockerfile: Fluxgym.dockerfile
  #   ports:
  #     - 7863:7860
  #   volumes:
  #     - flux-gym-data:/app/data
  #     - flux-gym-outputs/:/app/fluxgym/outputs/
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: all
  #     replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  # wd-llm-caption-cli:
  #   container_name: wd-llm-caption-cli-nvidia-docker
  #   image: wd-llm-caption-cli
  #   build:
  #     context: .
  #     dockerfile: Caption.dockerfile
  #   ports:
  #     - 7864:8282
  #   volumes:
  #     - wd-llm-caption-data:/home/appuser/data
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: all
  #     replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  # XXXX # kohya_ss with flux1 support
  # kohya-ss-gui:
  #   container_name: kohya-ss-gui-nvidia-docker
  #   image: kohya-ss-gui:sd3-flux.1
  #   build:
  #     context: .
  #     dockerfile: Kohya_ss.dockerfile
  #   ports:
  #     - 7863:7860
  #   environment:
  #     SAFETENSORS_FAST_GPU: 1
  #     TENSORBOARD_PORT: ${TENSORBOARD_PORT:-6006}
  #     PUID: 1000
  #     PGID: 1000
  #     TZ: Europe/Berlin
  #   volumes:
  #     - /home/kilone/space/dockerSpace/kohya_flux/data/:/app
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: all
  #     replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  # XXX # kohya_ss latest
  # kohya-ss-gui-latest:
  #   container_name: kohya-ss-gui-latest
  #   image: ghcr.io/bmaltais/kohya-ss-gui:latest
  #   user: 1000:0
  #   build:
  #     context: .
  #     args:
  #       - UID=1000
  #     cache_from:
  #       - ghcr.io/bmaltais/kohya-ss-gui:cache
  #     cache_to:
  #       - type=inline
  #   ports:
  #     - 7863:7860
  #   environment:
  #     SAFETENSORS_FAST_GPU: 1
  #     TENSORBOARD_PORT: ${TENSORBOARD_PORT:-6006}
  #   tmpfs:
  #     - /tmp
  #   volumes:
  #     - /tmp/.X11-unix:/tmp/.X11-unix
  #     - /home/kilone/space/dockerSpace/kohya_ss/dataset:/dataset
  #     - /home/kilone/space/dockerSpace/kohya_ss/dataset/images:/app/data
  #     - /home/kilone/space/dockerSpace/kohya_ss/dataset/logs:/app/logs
  #     - /home/kilone/space/dockerSpace/kohya_ss/dataset/outputs:/app/outputs
  #     - /home/kilone/space/dockerSpace/kohya_ss/dataset/regularization:/app/regularization
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/config:/app/config
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/user:/home/1000/.cache
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/triton:/home/1000/.triton
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/nv:/home/1000/.nv
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/keras:/home/1000/.keras
  #     - /home/kilone/space/dockerSpace/kohya_ss/.cache/config:/home/1000/.config # For backward compatibility
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: all
  #     replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  # tensorboard:
  #   container_name: tensorboard-nvidia-docker
  #   image: tensorflow/tensorflow:latest-gpu
  #   ports:
  #     # !Please change the port in .env file
  #     - ${TENSORBOARD_PORT:-6006}:6006
  #   volumes:
  #     - tensorboard-logs:/app/logs
  #   command: tensorboard --logdir=/app/logs --bind_all
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: all
  #     replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  #######################
  # reference containers
  #######################
  build-cuda122:
    build:
      context: .
      dockerfile: Cuda12.2.2.dockerfile
    container_name: build_cuda122
    networks:
      - skynet
    # Kein Starten des Containers: macht nichts au√üer Build
    entrypoint: ["/bin/true"]
    deploy:
      replicas: 0

  build-cuda118:
    build:
      context: .
      dockerfile: Cuda11.8.0.dockerfile
    container_name: build_cuda118
    networks:
      - skynet
    # Kein Starten des Containers: macht nichts au√üer Build
    entrypoint: ["/bin/true"]
    deploy:
      replicas: 0

networks:
  skynet:
    external: false

volumes:
  openWebUI-data:
  ollama-data:
  automatic1111-outputs:
  automatic1111-models:
  automatic1111-embeddings:
  comfyui-outputs:
  comfyui-models:
  roop-output:
  flux-gym-data:
  flux-gym-outputs:
  wd-llm-caption-data:
  tensorboard-logs: