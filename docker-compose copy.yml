#######################
#    ü§π SERVICES ü§π
#######################
# ‚úîÔ∏è ollama
# ‚úîÔ∏è openweb-ui
# üí© stable-deffusion automatic1111
# üí© comfyui
# üí© roop
# üí© kohya_ss
# üí© flux-gym
# üí© wd-llm-caption-cli
# üí© tensorboard
# wd-llm-caption-cli for better image captions
# text to speach /whisper / piper
# speach to text
# jupyter   docker run -d -p 8888:8888 --gpus all --name jupxter quay.io/jupyter/pytorch-notebook:cuda12-python-3.11.8
# home assistant support
#######################

#######################
#######################
## Prestuff on host required:
# curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
#  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
# sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
# sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
# sudo apt update
# sudo apt install -y nvidia-container-toolkit
#######################
## Configure NVIDIA Container Toolkit
# sudo nvidia-ctk runtime configure --runtime=docker
# sudo systemctl restart docker
#######################
## Test GPU integration
# docker run --gpus all nvidia/cuda:11.5.2-base-ubuntu20.04 nvidia-smi
#######################
#######################
# Using models
# llama3.2:latest
# (llama3.2-vision:latest)
# qwen2.5-coder:latest
# ?? nemotron: ??
# phi3.5:latest
# llava:latest
# nomic-embed-text:latest
# deepseek-coder-v2:latest
# stable-code:latest
# starcoder2:latest
# (codellama:latest)
# codestral:latest
# llama3:70b
# llama3
# (deepseek-v2:latest)
# (blackhat-hacker:latest)
# (whiterabbitneo:latest )
# orca-mini:latest
# (brxce/stable-diffusion-prompt-generator:latest)
#######################
#######################

version: '3.9'
services:
  openWebUI:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui-nvidia-docker
    hostname: openwebui
    networks:
      - skynet
    restart: unless-stopped
    volumes:
      - openWebUI-data:/app/backend/data
    ports:
      - 8080:8080
    environment:
      OLLAMA_BASE_URLS: http://ollama:11434
    depends_on:
      - ollama
    deploy:
      replicas: ${START_OLLAMA_RELATED_SERVICES:-0}

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-nvidia-docker
    hostname: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              count: all
      replicas: ${START_OLLAMA_RELATED_SERVICES:-0}
    networks:
      - skynet
    volumes:
      - ollama-data:/root/.ollama

  # latest script
  automatic1111:
    container_name: automatic1111-nvidia-docker
    image: automatic1111:latest
    restart: always
    build:
      context: .
      dockerfile: Automatic111.dockerfile
    ports:
      - "7860:7860"
    depends_on:
      - build-cuda122
    # environment:
    #   PUID: 1000
    #   PGID: 1000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              count: all
      replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}
    networks:
      - skynet
    volumes:
      - /mnt/e/AI/try/outputs:/home/appuser/stable-diffusion-webui/outputs
      - /mnt/e/AI/try/models:/home/appuser/stable-diffusion-webui/models/
      - /mnt/e/AI/try/embeddings:/home/appuser/stable-diffusion-webui/embeddings/

  comfyui:
    container_name: comfyui-nvidia-docker
    image: comfyui:latest
    restart: always
    build:
      context: .
      dockerfile: Confyui.dockerfile
    ports:
      - "7861:8188"
    depends_on:
      - build-cuda122
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              count: all
      replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}
    networks:
      - skynet
    volumes:
      - /mnt/e/AI/try/cf/outputs:/home/appuser/ComfyUI/outputs
      - /mnt/e/AI/try/cf/models:/home/appuser/ComfyUI/models/

  roop:
    container_name: roop-nvidia-docker
    image: roop:latest
    restart: always
    build:
      context: .
      dockerfile: Roop.dockerfile
    ports:
      - "7862:7860"
    depends_on:
      - build-cuda122
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              count: all
      replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}
    networks:
      - skynet
    volumes:
      - /home/kilone/space/dockerSpace/roop/output:/home/appuser/roop-unleashed/output

  flux-gym:
    container_name: flux-gym-nvidia-docker
    image: flux-gym-nvidia-docker
    build:
      context: .
      dockerfile: Fluxgym.dockerfile
    ports:
      - 7863:7860
    volumes:
      - /home/kilone/space/dockerSpace/kohya_flux/data/:/app/data
      - /home/kilone/space/dockerSpace/kohya_flux/outputs/:/app/fluxgym/outputs/
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
      replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  wd-llm-caption-cli:
    container_name: wd-llm-caption-cli-nvidia-docker
    image: wd-llm-caption-cli
    build:
      context: .
      dockerfile: Caption.dockerfile
    ports:
      - 7864:8282
    volumes:
      - /home/kilone/space/dockerSpace/kohya_flux/data/:/home/appuser/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
      replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  # kohya_ss with flux1 support
  kohya-ss-gui:
    container_name: kohya-ss-gui-nvidia-docker
    image: kohya-ss-gui:sd3-flux.1
    build:
      context: .
      dockerfile: Kohya_ss.dockerfile
    ports:
      - 7863:7860
    environment:
      SAFETENSORS_FAST_GPU: 1
      TENSORBOARD_PORT: ${TENSORBOARD_PORT:-6006}
      PUID: 1000
      PGID: 1000
      TZ: Europe/Berlin
    volumes:
      - /home/kilone/space/dockerSpace/kohya_flux/data/:/app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
      replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  # kohya_ss latest
  kohya-ss-gui-latest:
    container_name: kohya-ss-gui-latest
    image: ghcr.io/bmaltais/kohya-ss-gui:latest
    user: 1000:0
    build:
      context: .
      args:
        - UID=1000
      cache_from:
        - ghcr.io/bmaltais/kohya-ss-gui:cache
      cache_to:
        - type=inline
    ports:
      - 7863:7860
    environment:
      SAFETENSORS_FAST_GPU: 1
      TENSORBOARD_PORT: ${TENSORBOARD_PORT:-6006}
    tmpfs:
      - /tmp
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix
      - /home/kilone/space/dockerSpace/kohya_ss/dataset:/dataset
      - /home/kilone/space/dockerSpace/kohya_ss/dataset/images:/app/data
      - /home/kilone/space/dockerSpace/kohya_ss/dataset/logs:/app/logs
      - /home/kilone/space/dockerSpace/kohya_ss/dataset/outputs:/app/outputs
      - /home/kilone/space/dockerSpace/kohya_ss/dataset/regularization:/app/regularization
      - /home/kilone/space/dockerSpace/kohya_ss/.cache/config:/app/config
      - /home/kilone/space/dockerSpace/kohya_ss/.cache/user:/home/1000/.cache
      - /home/kilone/space/dockerSpace/kohya_ss/.cache/triton:/home/1000/.triton
      - /home/kilone/space/dockerSpace/kohya_ss/.cache/nv:/home/1000/.nv
      - /home/kilone/space/dockerSpace/kohya_ss/.cache/keras:/home/1000/.keras
      - /home/kilone/space/dockerSpace/kohya_ss/.cache/config:/home/1000/.config # For backward compatibility
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
      replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  tensorboard:
    container_name: tensorboard-nvidia-docker
    image: tensorflow/tensorflow:latest-gpu
    ports:
      # !Please change the port in .env file
      - ${TENSORBOARD_PORT:-6006}:6006
    volumes:
      - /home/kilone/space/dockerSpace/kohya_ss/dataset/tb/logs:/app/logs
    command: tensorboard --logdir=/app/logs --bind_all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
      replicas: ${START_IMAGE_MANIPULATION_SERVICES:-0}

  #######################
  # reference containers
  #######################
  build-cuda122:
    build:
      context: .
      dockerfile: Cuda12.2.2.dockerfile
    container_name: build_cuda122
    networks:
      - skynet
    # Kein Starten des Containers: macht nichts au√üer Build
    entrypoint: ["/bin/true"]
    deploy:
      replicas: 0

  build-cuda118:
    build:
      context: .
      dockerfile: Cuda11.8.0.dockerfile
    container_name: build_cuda118
    networks:
      - skynet
    # Kein Starten des Containers: macht nichts au√üer Build
    entrypoint: ["/bin/true"]
    deploy:
      replicas: 0

networks:
  skynet:
    external: false

volumes:
  openWebUI-data:
  ollama-data: